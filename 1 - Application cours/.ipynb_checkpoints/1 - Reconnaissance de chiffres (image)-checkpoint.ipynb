{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On charge la librairie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.vision.all import *"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On charge les informations contenues dans le dossier mnist_sample. Il contient des données d'entrainement et de validations représantant des 3 et des 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/storage/data/mnist_sample')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On applique la fonction ls() qui permet de lister les fichiers se trouvant dans le dossier (ici /train/3). La méthode sorted() permet de les trier dans l'ordre alphabétique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "threes = (path/'train'/'3').ls().sorted()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On peut afficher une image pour l'exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5klEQVR4nGNgoD9gRGJr+aSevsAw4Rc2demf/v379++fE1ZDhF78+/fv3793blhlM778e/Dv379e7A44/+/Sv3//lLBLhpz79+/fP00cbpe4+O/fv9VwLguyXLSeDgMDw1Fs2jSu/fr3D8VOJoSkpiLEmAKsNuZ9+/cPp52TbgswsEzmw+FYBgYGxoZ/t+VxSbL/+3dNBpdk179/JehiwhujGBgYGBgkP2AJviX/rturMBhHnvv3r5sDXdLy6L9/97Z8/Pfv71VuTLt6Mv/9+/fv3783yIIwf5aw8zAYRDJ8xB7TdAQABFdhZWAfWxoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F8427C0BF10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open(threes[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On transforme notre image en tableau de nombre (à plusieurs dimension). L'objet tensor va permettre de transformer l'image en tableau de pixel (0-255) niveaux de gris. On a donc un tableau de 28 par 28 pixel et pour chaque entrée un nombre entre 0 et 255 représentant le \"niveau de gris\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(threes[1])\n",
    "im3_t = tensor(image)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Il est possible de manipuler le tableau ainsi rendu. Par exemple ici on affiche le résultat des lignes 3 à 9 et des colonnes 3 à 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,  29],\n",
       "        [  0,   0,   0,  48, 166, 224],\n",
       "        [  0,  93, 244, 249, 253, 187],\n",
       "        [  0, 107, 253, 253, 230,  48],\n",
       "        [  0,   3,  20,  20,  15,   0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im3_t[4:10,4:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pour réaliser nos tests nous allons charger de cette manière l'ensemble des images dont les chemins ont été stockés dans la variable threes (point 3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_tensor = [tensor(Image.open(o)) for o in threes]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "J'obtiens donc une liste de 6131 objets tensor. Chaque objet est un tableau de 28 par 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 6131, torch.Tensor, torch.Size([28, 28]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(three_tensor), len(three_tensor), type(three_tensor[0]),three_tensor[0].shape "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Je cherches à regrouper l'ensemble de mes exemples (6131 ici) dans un seul tableau. Le fait de stocker l'ensemble des données dans un seul tensor va permettre d'éxécuter des opérations de manière parallèle (et efficace) pour gagner du temps. \n",
    "On en profite également pour ramener les valeures comprises en 0 et 255 à leurs équivalent entre 0 et 1 pour plus de simplicité dans les calculs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6131, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_threes = torch.stack(three_tensor).float()/255\n",
    "stacked_threes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1137],\n",
       "        [0.0000, 0.0000, 0.0000, 0.1882, 0.6510, 0.8784],\n",
       "        [0.0000, 0.3647, 0.9569, 0.9765, 0.9922, 0.7333],\n",
       "        [0.0000, 0.4196, 0.9922, 0.9922, 0.9020, 0.1882],\n",
       "        [0.0000, 0.0118, 0.0784, 0.0784, 0.0588, 0.0000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_threes[1,4:10,4:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On fait la même chose pour les 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/storage/data/mnist_sample')\n",
    "sevens = (path/'train'/'7').ls().sorted()\n",
    "seven_tensor = [tensor(Image.open(o)) for o in sevens]\n",
    "stacked_sevens = torch.stack(seven_tensor).float()/255"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On concatene toutes les images de trois et de sept dans le même tableau. D'abords tout les 3 puis tout les 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 28, 28])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_digits = torch.cat([stacked_threes, stacked_sevens])\n",
    "stacked_digits.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Au lieu d'avoir un tableau de 28 par 28 pour les pixels on préfère travailler avec une liste de chiffres mis bout à bout (784 du coup). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 784])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = stacked_digits.view(-1, 28*28)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On crée une liste de labels (3 ou 7) pour savoir à quoi chaque element correspond. Ici on positionne les labels 0 ou 1 car on cherche à savoir si l'image est un trois ou pas (un sept). \n",
    "Les x premiers element de mon tableau sont des trois et les y suivant sont des septs. x valant le nombre de trois dans mon jeux d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_labels = tensor([1]*len(threes) + [0]*len(sevens))\n",
    "stacked_labels.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On rajoute un axe pour simplifier l'accès au données label. (pas bien compris il semble qu'au lieu d'avoir un tableau de 0 ou 1 on a un tableau de tableau de 0 ou 1 ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1), tensor([1]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = stacked_labels.unsqueeze(1)\n",
    "stacked_labels[0],train_y[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A ce moment la nous avons un tableau train_x qui contient les 12396 images de 3 et 7 sous la forme d'une suite de nombres entre 0 et 1 représentant le contraste et un tableau train_y qui correspond à ce qu'il faut trouver (3 ou pas 3) pour chaque suite de nombre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12396, 784]), torch.Size([12396, 1]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape,train_y.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On peut appliquer des opérations sur l'ensemble des élements par exemple ici\n",
    "- On calcul le trois \"moyen\" en réalisant une moyenne pour chaque pixel de tout les éléments du tabeau \n",
    "- On calcul la \"distance\" entre une image donné et le trois moyen calculé précédement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f842798dd10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean3 = stacked_threes.mean(0)\n",
    "show_image(mean3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.axes._subplots.AxesSubplot at 0x7f8427177390>,\n",
       " <matplotlib.axes._subplots.AxesSubplot at 0x7f842712bf90>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a_3 = stacked_threes[1]\n",
    "dist_3_abs = (a_3 - mean3).abs()\n",
    "dist_3_sqr = (a_3 - mean3)**2\n",
    "\n",
    "show_image(dist_3_abs), show_image(dist_3_sqr)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "il est également possible d'effectuer des opérations sur l'ensemble d'un tensor. Par exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3, 4],\n",
       "        [5, 6, 7]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_example = tensor([1,2,3],[4,5,6])\n",
    "tensor_example + 1 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cela va nous permettre de définir des fonctions que nous allons pouvoir éxécuter sur l'ensemble des objets d'un tensor. \n",
    "- On décrit une fonction de calcul de la distance (on trouve la valeur absolue de distance entre chaque pixel puis on resort la moyenne de ces distances. \n",
    "- On teste la différence entre un 3 et le 3 moyen puis entre un 7 et le 3 moyen.\n",
    "- On peut le faire sur l'ensemble de la liste d'un seul coup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_distance(a,b):\n",
    "    return (a-b).abs().mean((-1,-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1114), tensor(0.1854))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_distance(stacked_threes[1],mean3),mnist_distance(stacked_sevens[1],mean3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1074, 0.1114, 0.1100,  ..., 0.1316, 0.1220, 0.1267])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_distance(stacked_threes,mean3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On créer une fonction mathématique fléxible qui va nous permettre pour chaque image d'essayer de déterminer le label. \n",
    "L'idée ici va être de multiplier chaque pixel (niveau de gris) par un paramètre différent et d'additionner l'ensemble des résultat. Soit\n",
    "pixel0 x param0 + pixel1 x param1 + ...\n",
    "\n",
    "On pense que cette combinaison linéaire va permettre de reconnaitre un chiffre car le modèle va apparendre des paramètres très grands sur les pixels très probables pour un 3 et très négatifs pour les pixel imbrobables pour un trois. Ainsi la somme sera fortement supérieur à 0 si on a beaucoup de pixel probable pour un trois et inversement (très négatif si improbable). "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On crée une fonction pour initialiser les 784 paramètres (chiffres entre -1 et 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(size, std=1.0):\n",
    "    return (torch.randn(size)*std).requires_grad_()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On initialise une liste de 784 paramètre aléatoires ainsi qu'une constante (biais). Le biais permet de s'aligner plus facilement sur 0 (pas compris pourquoi). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 1]), torch.Size([1]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights=init_params((28*28,1))\n",
    "bias= init_params(1)\n",
    "weights.shape,bias.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On créer la fonction de calcul du model. La multiplication s'effectue directement sur le tensor (tensor*tensor) grace à l'opérateur @. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(xb):\n",
    "    return xb@weights + bias"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On créer la fonction de prédiction. Elle retourne un booléen qui indique si la prédiction réalisé par le modèle pour chaque élément est >0 ou <0. Cette fonction sera utilisée uniquement pour le calcul de la performance (résultat binaire) pour l'entrainement on utilisera une fonction de cout avec une notion de distance par rapport au résultat souhaité. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(xb):\n",
    "    return linear_model(xb) > 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On utilise le jeux d'entrainement et on crée une première prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12396, 1]),\n",
       " tensor([[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         ...,\n",
       "         [False],\n",
       "         [ True],\n",
       "         [False]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = predict(train_x)\n",
    "predictions.shape,predictions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On va cherche maintenant à améliorer notre prédiction (pour le moment basé sur un jeux de paramètre initialisé aléatoirement). On va donc chercher à les ajuster pour améliorer la prédiction. Pour cela on va créer une fonction qui messure l'erreur de prédiction. \n",
    "On détermine pour chaque prédictions si elle est correcte en la comparant à notre tableau de label (3 ou pas 3). Puis on transforme le résultat en nombre pour obtenir un tableau de 0 et de 1 (1 pour chaque prédiction juste). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(activations, targets):\n",
    "    predictions = activations > 0\n",
    "    corrects = (predictions -- targets)\n",
    "    return corrects.float().mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cette métrique ne peut pas être utilisé par le machine learning pour s'entraîner car le résultat est binaire (bon/pas bon). Nous allons chercher une méthode permettant de savoir si le changement nous a fait nous rapprocher d’un bon résultat même s’il est toujours mauvais. \n",
    "Pour cela nous allons créer une fonction de cout qui va utiliser la notion de distance (plus ou moins proche du bon résultat)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cette fonction utilise la méthode mathématique sigmoid qui permet de projeter les valeurs entre 0 et 1. Ensuite la distances avec le bon résultat est 1-resultat lorsque le bon résultat est 1 et resultat-0 lorsque le bon résultat est 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(activations_01, targets):\n",
    "        activations_01 = activations_01.sigmoid()\n",
    "        distances = torch.where(targets==1,1-activations_01,activations_01)\n",
    "        return distances.mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Exemples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.7109, 0.5987, 0.5498, 1.0000, 0.0000]), tensor(132.4600))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = tensor([1,0,1,0,1])\n",
    "activations_01 = tensor([0.9,0.4,0.2,100,-560])\n",
    "activations_01.sigmoid(),mnist_loss(activations_01,labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Nous allons mettre en place le principe de mini-batch afin de tester les paramètres et de pouvoir les ajuster pour entrainer le modèle par lot de x exemples plutot que sur l'ensenble du jeux d'entrainement afin d'optimiser le traitement et de bénéficier d'un plus grand nombre de \"pas\" d'ajustement. \n",
    "Pour cela on utilise la classe DataLoader.\n",
    "Exemple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 4, 10,  1]), tensor([8, 5, 0]), tensor([7, 3, 2]), tensor([6])]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll = tensor([1,2,3,4,5,6,7,8,0,10])\n",
    "dl = DataLoader(coll,batch_size=3,shuffle=True)\n",
    "list(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Avec des couples valeur / label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7, 'h'),(8, 'i'),(9, 'j')...],\n",
       " [(tensor([ 5,  2, 15, 25, 22, 12]), ('f', 'c', 'p', 'z', 'w', 'm')),\n",
       "  (tensor([ 1,  6,  7, 11, 21, 17]), ('b', 'g', 'h', 'l', 'v', 'r')),\n",
       "  (tensor([ 4,  9, 13, 24, 16, 18]), ('e', 'j', 'n', 'y', 'q', 's')),\n",
       "  (tensor([ 0,  8, 14,  3, 23, 20]), ('a', 'i', 'o', 'd', 'x', 'u')),\n",
       "  (tensor([10, 19]), ('k', 't'))])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasource = L(enumerate(string.ascii_lowercase))\n",
    "dl = DataLoader(datasource,batch_size=6,shuffle=True)\n",
    "datasource,list(dl)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Nous avons désormais toutes les données nécessaires à l'entrainement. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Les paramètres que nous initialisons: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = init_params((28*28,1))\n",
    "bias = init_params(1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Les tensors contenant les images (liste de pixel) et les labels attendus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12396, 784]), torch.Size([12396, 1]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape,train_y.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On crée un dataset en utilisant la fonction python zip pour itérer sur les deux listes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12396, torch.Size([784]), tensor([1]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = list(zip(train_x,train_y))\n",
    "x,y = train_dataset[0]\n",
    "len(train_dataset),x.shape,y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On créer nos mini-batch avec la classe DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl = DataLoader(train_dataset,batch_size=256)\n",
    "xb,yb = first(train_dl)\n",
    "xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On calcul les activations grace à notre méthode linea_model qui appliquent l'ensemble des paramètres à notre model. \n",
    "On prend uniquement les 4 premiers pour l'exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.8473],\n",
       "        [-0.4300],\n",
       "        [ 7.7864],\n",
       "        [ 5.2126]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb,yb = first(train_dl)\n",
    "activations = linear_model(xb[:4])\n",
    "activations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On calcul notre erreur avec notre méthode mnist_loss définie ci-dessus (application sigmoid + renvoit de la moyenne des distances avec le résultat attendu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3977, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mnist_loss(activations,yb[:4])\n",
    "loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On calcul le gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 1]), tensor(-0.0095), tensor([-0.0663]))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "weights.grad.shape,weights.grad.mean(),bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "On rassemble le tout dans une fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(xb, yb, model): \n",
    "    predictions = model(xb)\n",
    "    loss = mnist_loss(predictions, yb)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On peut désormais créer une fonction d'entrainement d'une epoque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, lr, params)\n",
    "    # ON parcours le mini-batchs\n",
    "    for xb, yb in train_dl\n",
    "        calc_grad(xb,yb,model)\n",
    "        for p in params\n",
    "            p.data -= p.grad * lr # On modifie chaque paramètre en fonction du gradient et du learning rate défini\n",
    "            p.grad.zero_() # remise à zéros du gradient pour itération suivante. \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
